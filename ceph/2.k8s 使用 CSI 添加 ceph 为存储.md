### k8s 1.25.0 CSI 模式添加 ceph 17 为为外部存储（动态存储卷）

#### 1.动态持久卷
不需要存储管理员干预，使k8s使用的存储image创建自动化，即根据使用需要可以动态申请存储空间并自动创建。需要先定义一个或者多个StorageClass，每个StorageClass都必须配置一个provisioner，用来决定使用哪个卷插件分配PV。然后，StorageClass资源指定持久卷声明请求StorageClass时使用哪个provisioner来在对应存储创建持久卷。

#### 2.创建一个普通用户来给k8s做rdb的映射
在ceph集群中创建一个k8s专用的pool和用户：
```shell
ceph osd pool create kubernetes 16 16
ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'

初始化
rbd pool init kubernetes

查看 pool
ceph osd pool ls 
rados lspools
```
得到
```shell
[client.kubernetes]
	key = AQDTAQRkqBzJHRAAodEbo/7BBEa5mZcThSQU8Q==
```
后面的配置需要用到这里的 key，如果忘了可以通过以下命令来获取：
```shell
ceph auth get client.kubernetes
```
得到
```shell
[client.kubernetes]
	key = AQDTAQRkqBzJHRAAodEbo/7BBEa5mZcThSQU8Q==
	caps mgr = "profile rbd pool=kubernetes"
	caps mon = "profile rbd"
	caps osd = "profile rbd pool=kubernetes"
exported keyring for client.kubernetes
```
#### 3. 部署 ceph-csi 在k8s master 集群上
拉取 ceph-csi 的 最新 release 分支（v3.8.0）
```shell
git clone --depth 1 --branch v3.8.0 https://github.com/ceph/ceph-csi
```
##### 修改 Configmap
获取 Ceph 集群的信息：
```shell
[root@ceph01 ~]# ceph mon dump
epoch 3
fsid 756da094-baac-11ed-b1c8-05cc32ab3be6
last_changed 2023-03-05T02:21:50.027681+0000
created 2023-03-04T16:50:15.571160+0000
min_mon_release 17 (quincy)
election_strategy: 1
0: [v2:192.168.0.180:3300/0,v1:192.168.0.180:6789/0] mon.ceph01
1: [v2:192.168.0.181:3300/0,v1:192.168.0.181:6789/0] mon.ceph02
2: [v2:192.168.0.182:3300/0,v1:192.168.0.182:6789/0] mon.ceph03
dumped monmap epoch 3
```

这里需要用到两个信息：
- fsid : 这个是 Ceph 的集群 ID。
- 监控节点信息。目前 ceph-csi 只支持 v1 版本的协议，所以监控节点那里我们只能用 v1 的那个 IP 和端口号（例如，192.168.0.180:6789）。

进入 ceph-csi 的 deploy/rbd/kubernetes 目录：
```shell
[root@master ~]# cd ceph-csi/deploy/rbd/kubernetes
[root@master kubernetes]# ls -l ./
total 40
-rw-r--r-- 1 root root  309 Aug 12 20:18 csi-config-map.yaml
-rw-r--r-- 1 root root  435 Aug 12 20:18 csidriver.yaml
-rw-r--r-- 1 root root 1776 Aug 12 20:18 csi-nodeplugin-psp.yaml
-rw-r--r-- 1 root root 1110 Aug 12 20:18 csi-nodeplugin-rbac.yaml
-rw-r--r-- 1 root root 1199 Aug 12 20:18 csi-provisioner-psp.yaml
-rw-r--r-- 1 root root 3264 Aug 12 20:18 csi-provisioner-rbac.yaml
-rw-r--r-- 1 root root 8021 Aug 12 20:18 csi-rbdplugin-provisioner.yaml
-rw-r--r-- 1 root root 7242 Aug 12 20:18 csi-rbdplugin.yaml
```
将以上获取的信息写入 csi-config-map.yaml：
vi csi-config-map.yaml
```shell
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "756da094-baac-11ed-b1c8-05cc32ab3be6",
        "monitors": [
          "192.168.0.180:6789",
          "192.168.0.181:6789",
          "192.168.0.182:6789"
        ]
      }
    ]    
metadata:
  name: ceph-csi-config
```
将此 Configmap 存储到 Kubernetes 集群中：
```shell
kubectl  apply -f csi-config-map.yaml
```

创建ceph-config
```shell
cat <<EOF > ceph-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  ceph.conf: |
    [global]
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  # keyring is a required key and its value should be empty
  keyring: |
metadata:
  name: ceph-config
EOF
```
创建
```shell
kubectl apply -f ceph-config-map.yaml
```

创建 ceph-csi-encryption-kms-config
```shell
cat <<EOF > csi-kms-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    {}
metadata:
  name: ceph-csi-encryption-kms-config
EOF
```
启用
```shell
kubectl apply -f csi-kms-config-map.yaml
```

##### 新建 Secret
使用创建的 kubernetes 用户 ID 和 cephx 密钥生成 Secret：
```shell
cat <<EOF > csi-rbd-secret.yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: csi-rbd-secret
     namespace: default
   stringData:
     userID: kubernetes
     userKey: AQDTAQRkqBzJHRAAodEbo/7BBEa5mZcThSQU8Q==
EOF
```
部署 Secret：
```shell
kubectl apply -f csi-rbd-secret.yaml
```
##### RBAC 授权
创建必须的 ServiceAccount 和 RBAC ClusterRole/ClusterRoleBinding 资源对象：
```shell
kubectl create -f csi-provisioner-rbac.yaml
kubectl create -f csi-nodeplugin-rbac.yaml
```

创建 PodSecurityPolicy：
```shell
kubectl create -f csi-provisioner-psp.yaml
kubectl create -f csi-nodeplugin-psp.yaml
```

##### 部署 CSI sidecar
将 csi-rbdplugin-provisioner.yaml 和 csi-rbdplugin.yaml 中的 kms 部分配置注释掉：
```shell
           # - name: ceph-csi-encryption-kms-config
           #   mountPath: /etc/ceph-csi-encryption-kms-config/

.... 
        #- name: ceph-csi-encryption-kms-config
        #  configMap:
        #    name: ceph-csi-encryption-kms-config
          
```
注释掉pod亲和性
```shell
#      affinity:
#        podAntiAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#            - labelSelector:
#                matchExpressions:
#                  - key: app
#                    operator: In
#                    values:
#                      - csi-rbdplugin-provisioner
#              topologyKey: "kubernetes.io/hostname"
```

改为一个副本
```shell
spec:
  replicas: 1
```

部署 csi-rbdplugin-provisioner：
```
kubectl create -f csi-rbdplugin-provisioner.yaml 
```
这里面包含了 6 个 Sidecar 容器，包括 external-provisioner、external-attacher、csi-resizer 和 csi-rbdplugin。

##### 部署 RBD CSI driver
最后部署 RBD CSI Driver：
``` 
kubectl create -f csi-rbdplugin.yaml
```
Pod 中包含两个容器：CSI node-driver-registrar 和 CSI RBD driver。
##### 创建 Storageclass
```shell
cat <<EOF > storageclass.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: 756da094-baac-11ed-b1c8-05cc32ab3be6
   pool: kubernetes
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
   csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard
EOF
```
创建 storageclass 
```shell
 kubectl apply -f storageclass.yaml 
 
 kubectl get sc
NAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   31s
```

- 这里的 clusterID 对应之前步骤中的 fsid。
- imageFeatures 用来确定创建的 image 特征，如果不指定，就会使用 RBD 内核中的特征列表，但 Linux 不一定支持所有特征，所以这里需要限制一下。

#### 4.试用 ceph-csi
Kubernetes 通过 PersistentVolume 子系统为用户和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来，其中 PV（PersistentVolume） 是实际的存储，PVC（PersistentVolumeClaim） 是用户对存储的请求。

下面通过官方仓库的示例来演示如何使用 ceph-csi。

先进入 ceph-csi 项目的 example/rbd 目录，然后直接创建 PVC：
```shell
kubectl apply -f pvc.yaml
```
查看 PVC 和申请成功的 PV：
```shell
$ kubectl get pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
rbd-pvc   Bound    pvc-44b89f0e-4efd-4396-9316-10a04d289d7f   1Gi        RWO            csi-rbd-sc     8m21s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE
pvc-44b89f0e-4efd-4396-9316-10a04d289d7f   1Gi        RWO            Delete           Bound    default/rbd-pvc      csi-rbd-sc              8m18s
```
再创建示例 Pod：
```
kubectl apply -f pod.yaml
```

进入 Pod 里面测试读写数据：
```shell
kubectl apply -f pod.yaml
```
进入 Pod 里面测试读写数据：
```shell

kubectl exec -it csi-rbd-demo-pod bash
root@csi-rbd-demo-pod:/# cd /var/lib/www/
root@csi-rbd-demo-pod:/var/lib/www# ls -l
total 4
drwxrwxrwx 3 root root 4096 Sep 14 09:09 html
root@csi-rbd-demo-pod:/var/lib/www# echo "你好，扶风！" > fufeng.txt
root@csi-rbd-demo-pod:/var/lib/www# cat fufeng.txt
你好，扶风！
```
列出 kubernetes pool 中的 rbd images：
```shell
rbd ls -p kubernetes
csi-vol-28a12c83-e06f-4711-9511-4b011c5fdc82
```

查看该 image 的特征：

```shell
[root@ceph01 ceph]# rbd info csi-vol-28a12c83-e06f-4711-9511-4b011c5fdc82 -p kubernetes
rbd image 'csi-vol-28a12c83-e06f-4711-9511-4b011c5fdc82':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 3978b780dd4b
	block_name_prefix: rbd_data.3978b780dd4b
	format: 2
	features: layering
	op_features: 
	flags: 
	create_timestamp: Sun Mar  5 11:14:46 2023
	access_timestamp: Sun Mar  5 11:14:46 2023
	modify_timestamp: Sun Mar  5 11:14:46 2023
```

到此，k8s 使用外部 ceph 存储配置结束